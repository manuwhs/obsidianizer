{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from TexSoup import TexSoup\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from obsidianizer.latex_tools.utils import load_drafts_entries, save_cleaned_sentences_to_latex, print_differences_in_journals\n",
    "from obsidianizer.latex_tools.journal_processing import get_sentences\n",
    "from obsidianizer.nlp.bow import generate_word_cloud_image\n",
    "from obsidianizer.latex_tools.plots import get_statistics_email_draft\n",
    "from obsidianizer.nlp.translation import get_translator, get_journal_translator\n",
    "import datetime as dt\n",
    "from obsidianizer.nlp.text_cleanup import n_grams_function\n",
    "from obsidianizer.obsidian.journal_tools import create_obsidian_files_from_journal\n",
    "\n",
    "from obsidianizer.nlp.text_cleanup import get_most_used_words, remove_stop_words\n",
    "from obsidianizer.obsidian.vault import load_vault\n",
    "\n",
    "\n",
    "from obsidianizer.nlp.text_cleanup import filter_entries_by_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load item email drafts from file\n",
    "\n",
    "In the following it is shown how to load the items generated by the email function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../../knowledge/Randiary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = load_drafts_entries(filepath)\n",
    "journal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = journal_df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess entries\n",
    "\n",
    "We need to preprocess the sentences properly. This includes:\n",
    "- Dividing the entry text into sentences.\n",
    "- Autocorrect words (no matter how bad this is).\n",
    "- Translate into a common language (English)\n",
    "- Tokenization of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = get_sentences(journal_df)\n",
    "journal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = filter_entries_by_languages(journal_df, [\"en\"], mode  = \"all\")\n",
    "journal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "x_train = list(itertools.chain.from_iterable(journal_df[\"sentences\"]))\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "we will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obsidianizer.nlp.tokenizers import nltk_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df[\"tokenized_sentences\"] = journal_df[\"sentences\"].apply(nltk_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df[\"tokenized_sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def build_lda(x_train, num_of_topic=10):\n",
    "    vec = CountVectorizer()\n",
    "    transformed_x_train = vec.fit_transform(x_train)\n",
    "    feature_names = vec.get_feature_names()\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_of_topic, max_iter=5, learning_method=\"online\", random_state=0)\n",
    "    lda.fit(transformed_x_train)\n",
    "\n",
    "    return lda, vec, feature_names\n",
    "\n",
    "\n",
    "def display_word_distribution(model, feature_names, n_word):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        words = []\n",
    "        for i in topic.argsort()[: -n_word - 1 : -1]:\n",
    "            words.append(feature_names[i])\n",
    "        print(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model, vec, feature_names = build_lda(x_train)\n",
    "display_word_distribution(model=lda_model, feature_names=feature_names, n_word=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "\n",
    "df = px.data.iris()\n",
    "\n",
    "features = df.loc[:, :'petal_width']\n",
    "\n",
    "umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "umap_3d = UMAP(n_components=3, init='random', random_state=0)\n",
    "\n",
    "proj_2d = umap_2d.fit_transform(features)\n",
    "proj_3d = umap_3d.fit_transform(features)\n",
    "\n",
    "fig_2d = px.scatter(\n",
    "    proj_2d, x=0, y=1,\n",
    "    color=df.species, labels={'color': 'species'}\n",
    ")\n",
    "fig_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "    color=df.species, labels={'color': 'species'}\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "fig_2d.show()\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "df = px.data.iris()\n",
    "\n",
    "features = df.loc[:, :'petal_width']\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "projections = tsne.fit_transform(features)\n",
    "\n",
    "fig = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    color=df.species, labels={'color': 'species'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the spacy model that you have installed\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# process a sentence using the model\n",
    "doc = nlp(\"This is some text that I am processing with Spacy\", )\n",
    "# It's that simple - all of the vectors and words are assigned after this point\n",
    "# Get the vector for 'text':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word_embeddings = np.stack([word.vector for word in doc], axis = 1)\n",
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[3].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
