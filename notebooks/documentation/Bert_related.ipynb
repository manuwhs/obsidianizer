{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from TexSoup import TexSoup\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from obsidianizer.latex_tools.utils import load_drafts_entries, save_cleaned_sentences_to_latex, print_differences_in_journals\n",
    "from obsidianizer.latex_tools.journal_processing import get_sentences\n",
    "from obsidianizer.nlp.bow import generate_word_cloud_image\n",
    "from obsidianizer.latex_tools.plots import get_statistics_email_draft\n",
    "from obsidianizer.nlp.translation import get_translator, get_journal_translator\n",
    "import datetime as dt\n",
    "from obsidianizer.nlp.text_cleanup import n_grams_function\n",
    "from obsidianizer.obsidian.journal_tools import create_obsidian_files_from_journal\n",
    "\n",
    "from obsidianizer.nlp.text_cleanup import get_most_used_words, remove_stop_words\n",
    "from obsidianizer.obsidian.vault import load_vault\n",
    "import itertools\n",
    "\n",
    "from obsidianizer.nlp.text_cleanup import filter_entries_by_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    cuda_core = torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load item email drafts from file\n",
    "\n",
    "In the following it is shown how to load the items generated by the email function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../../knowledge/Randiary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = load_drafts_entries(filepath)\n",
    "journal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = journal_df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess entries\n",
    "\n",
    "We need to preprocess the sentences properly. This includes:\n",
    "- Dividing the entry text into sentences.\n",
    "- Autocorrect words (no matter how bad this is).\n",
    "- Translate into a common language (English)\n",
    "- Tokenization of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = get_sentences(journal_df)\n",
    "journal_df = filter_entries_by_languages(journal_df, [\"en\"], mode  = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(itertools.chain.from_iterable(journal_df[\"sentences\"]))\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# queries are stored in the variable query_data_train\n",
    "# correct intent labels are stored in the variable labels\n",
    "#\n",
    "\n",
    "# add special tokens for BERT to work properly\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in x_train]\n",
    "print(sentences[0])\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "features = sentence_embeddings\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=0)\n",
    "projections = tsne.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "umap_3d = UMAP(n_components=3, init='random', random_state=0)\n",
    "projections = umap_3d.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame()\n",
    "sentences_df[\"sentence\"] = x_train\n",
    "sentences_df[\"projection_x\"] = projections[:,0]\n",
    "sentences_df[\"projection_y\"] = projections[:,1]\n",
    "sentences_df[\"projection_z\"] = projections[:,2]\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(sentences_df, x=\"projection_x\", y=\"projection_y\",\n",
    "                 hover_data=[\"sentence\"])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3d = px.scatter_3d(\n",
    "    sentences_df, x=\"projection_x\", y=\"projection_y\", z=\"projection_z\",\n",
    "    hover_data=[\"sentence\"],\n",
    ")\n",
    "fig_3d.update_traces(marker_size=2)\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "\n",
    "df = px.data.iris()\n",
    "\n",
    "features = df.loc[:, :'petal_width']\n",
    "\n",
    "umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "umap_3d = UMAP(n_components=3, init='random', random_state=0)\n",
    "\n",
    "proj_2d = umap_2d.fit_transform(features)\n",
    "proj_3d = umap_3d.fit_transform(features)\n",
    "\n",
    "fig_2d = px.scatter(\n",
    "    proj_2d, x=0, y=1,\n",
    "    color=df.species, labels={'color': 'species'}\n",
    ")\n",
    "fig_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "    color=df.species, labels={'color': 'species'}\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "fig_2d.show()\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_detection(embeddings, threshold=0.75, min_community_size=10, init_max_size=1000):\n",
    "    \"\"\"\n",
    "    Function for Fast Community Detection\n",
    "    Finds in the embeddings all communities, i.e. embeddings that are close (closer than threshold).\n",
    "    Returns only communities that are larger than min_community_size. The communities are returned\n",
    "    in decreasing order. The first element in each list is the central point in the community.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute cosine similarity scores\n",
    "    cos_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "    # Minimum size for a community\n",
    "    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)\n",
    "\n",
    "    # Filter for rows >= min_threshold\n",
    "    extracted_communities = []\n",
    "    for i in range(len(top_k_values)):\n",
    "        if top_k_values[i][-1] >= threshold:\n",
    "            new_cluster = []\n",
    "\n",
    "            # Only check top k most similar entries\n",
    "            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=True)\n",
    "            top_idx_large = top_idx_large.tolist()\n",
    "            top_val_large = top_val_large.tolist()\n",
    "\n",
    "            if top_val_large[-1] < threshold:\n",
    "                for idx, val in zip(top_idx_large, top_val_large):\n",
    "                    if val < threshold:\n",
    "                        break\n",
    "\n",
    "                    new_cluster.append(idx)\n",
    "            else:\n",
    "                # Iterate over all entries (slow)\n",
    "                for idx, val in enumerate(cos_scores[i].tolist()):\n",
    "                    if val >= threshold:\n",
    "                        new_cluster.append(idx)\n",
    "\n",
    "            extracted_communities.append(new_cluster)\n",
    "\n",
    "    # Largest cluster first\n",
    "    extracted_communities = sorted(extracted_communities, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "    # Step 2) Remove overlapping communities\n",
    "    unique_communities = []\n",
    "    extracted_ids = set()\n",
    "\n",
    "    for community in extracted_communities:\n",
    "        add_cluster = True\n",
    "        for idx in community:\n",
    "            if idx in extracted_ids:\n",
    "                add_cluster = False\n",
    "                break\n",
    "\n",
    "        if add_cluster:\n",
    "            unique_communities.append(community)\n",
    "            for idx in community:\n",
    "                extracted_ids.add(idx)\n",
    "\n",
    "    return unique_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "print(\"Start clustering\")\n",
    "start_time = time.time()\n",
    "\n",
    "#Two parameter to tune:\n",
    "#min_cluster_size: Only consider cluster that have at least 25 elements (30 similar sentences)\n",
    "#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\n",
    "clusters = community_detection(sentence_embeddings, min_community_size=10, threshold=0.81)\n",
    "\n",
    "\n",
    "#Print all cluster / communities\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(\"\\nCluster {}, #{} Elements \".format(i+1, len(cluster)))\n",
    "    for sentence_id in cluster:\n",
    "        print(\"\\t\", x_train[sentence_id])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform kmean clustering\n",
    "clustering_model = AgglomerativeClustering(n_clusters=10, distance_threshold=None) #, affinity='cosine', linkage='average', distance_threshold=0.4)\n",
    "clustering_model.fit(sentence_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(x_train[sentence_id])\n",
    "\n",
    "for i, cluster in clustered_sentences.items():\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df[\"color\"] = cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3d = px.scatter_3d(\n",
    "    sentences_df, x=\"projection_x\", y=\"projection_y\", z=\"projection_z\",\n",
    "    hover_data=[\"sentence\"], color = \"color\"\n",
    ")\n",
    "fig_3d.update_traces(marker_size=2)\n",
    "fig_3d.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
