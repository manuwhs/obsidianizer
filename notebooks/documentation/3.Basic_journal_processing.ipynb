{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic journal entries processing\n",
    "\n",
    "This notebooks shows a set of functionalities to process the diary entries:\n",
    "- Loading and writing to disk.\n",
    "- Divide into sentences.\n",
    "- Sentences processing: Autocorrection, Translation, Lemmatization. \n",
    "- Words processing: N-grams, bow,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from TexSoup import TexSoup\n",
    "import glob\n",
    "import pandas as pd\n",
    "import nlu\n",
    "import datetime as dt\n",
    "\n",
    "from obsidianizer.latex_tools.utils import load_drafts_entries, save_cleaned_sentences_to_latex, print_differences_in_journals\n",
    "from obsidianizer.latex_tools.journal_processing import get_sentences\n",
    "from obsidianizer.nlp.bow import generate_word_cloud_image\n",
    "from obsidianizer.latex_tools.plots import get_statistics_email_draft\n",
    "from obsidianizer.nlp.translation import get_translator, get_journal_translator\n",
    "\n",
    "from obsidianizer.nlp.text_cleanup import n_grams_function\n",
    "from obsidianizer.nlp.text_cleanup import get_most_used_words, remove_stop_words\n",
    "\n",
    "from obsidianizer.nlp.auto_correction import get_misspelled_words, correct_sentence, get_candidates\n",
    "from obsidianizer.nlp.contextual_auto_correction import create_contextual_spell_check\n",
    "from obsidianizer import EXAMPLE_JOURNAL_PATH, EXAMPLE_CLEANED_JOURNAL_PATH, JOURNALS_PATH\n",
    "from obsidianizer.journal.cleaning import load_journals_splitted_by_language, get_journal_splits_by_language_filepaths, split_journal_by_language,write_journal_splits_by_language_to_latex\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load journal entries from latex file.\n",
    "\n",
    "In the following it is shown how to load the items generated by the email function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = EXAMPLE_JOURNAL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load draft entries\n",
    "\n",
    "The draft entries have as columns:\n",
    "- datetime_str: The string datetime created from parse_dated_comment_to_latex_item\n",
    "- entry_text: The text writen in the email draft (or more generally in the comment)\n",
    "- datetime: The datetime_str transformed to datetime object\n",
    "- date: The date of the text to serve as a groupby \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entries = load_drafts_entries(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Get the sentences in the entries\n",
    "\n",
    "We need to preprocess the sentences properly.\n",
    "- It splits the entries into sentences.\n",
    "- It detects the language of the sentences.\n",
    "- It counds the number of words and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = get_sentences(data_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Save cleaned journal to disk\n",
    "\n",
    "We will for sure have to clean a lot from the comments such as:\n",
    "- Deleting meaningless entries (numbers and other shit I might have put there just to remember)\n",
    "- Trimming sentences: Removing unnecesary new lines and spaces.\n",
    "- Correcting words: There is usually a lot of misspelled words that we should fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cleaned_journal = EXAMPLE_CLEANED_JOURNAL_PATH\n",
    "text_output_journal = save_cleaned_sentences_to_latex(journal_df, output_cleaned_journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entries_2 = load_drafts_entries(output_cleaned_journal)\n",
    "data_entries_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df_2 = get_sentences(data_entries_2)\n",
    "journal_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare that the sentences are the same\n",
    "\n",
    "There seems to be some slight differences between the original drafts and what we save to disk, due to special characters, comments it seems. Lines related to \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_indices,weird_sentence_within_index = print_differences_in_journals(journal_df, journal_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentences processing\n",
    "\n",
    "The next subsections contain a list of different transformations of the sentences in the entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Autocorrect words\n",
    "\n",
    "Since the OCR of the pdfs or our journal entries usually contain typos, we have implemented some automatic correction of words. Their precision is not great so use them with skepticism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"To die having experience the sweetness of dying, without dead, that is where conciousness and reality rejoyce, rejoyce in the destruction of the self, the ego, the eye, the judgement, the look for meaning. If meaning were to exist, that would be universe in-it-self. The thing in it-self. Not that far from Kand, buddism and stoicism. But that is what it is, the pleasure of the dissolution of the ego, and maybe that is for many, the best way to live. I am not ashamed to admit that maybe that would be a good picture for whoever can actually believe in it 100%, but I cannot, and consciousness reveals againt it, maybe driven by fear? Maybe consicousness was born out of fear, maybe black creates blue, chaos creates order. And in the end, the highest pleasure is the dissapearance of one-self. Which in my opinion one can only allow unti lthe skeptic wihin us is satisfied, it feels it is not being deceived, by others and us. The skiptic of trust, of suspension of jusgement, the lack of agreeblemenss. But that also is necessary, it is necesaary because of deceive, danger, betrail, because of the harshness of nature, the will to power of others, the cosmic dance between trust to become nothing, to give up the self, and the will to power. The conquer of knowledge, the facing of the dark to get tools. Welcome to my fucked up mind :)\"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic single word autocorrection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the misspeled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled_words = get_misspelled_words(sentence)\n",
    "misspelled_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the most likely candidates to replace the words with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_misspelled = get_candidates(misspelled_words)\n",
    "candidates_misspelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct the misspelled words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentece_corrected= correct_sentence(sentence)\n",
    "sentece_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual autocorrection\n",
    "\n",
    "Based on spacy pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_corrector = create_contextual_spell_check(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = auto_corrector(sentence)\n",
    "doc._.outcome_spellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.suggestions_spellCheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Translation\n",
    "\n",
    "We can translate sentences into each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_translator = get_journal_translator(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df[\"sentences_translated\"] = journal_df[[\"sentences\",\"languages\"]].apply(journal_translator, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = nlu.load(\"en.embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.predict(\"Hello my friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 n_grams\n",
    "\n",
    "Get the most common n_grams in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ngrams = 3\n",
    "n_grams_df = n_grams_function(journal_df.iloc[:1000], column = \"sentences\",n = n_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Get the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used_words = get_most_used_words(journal_df) \n",
    "most_used_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_used_words_cleaned = remove_stop_words(most_used_words)\n",
    "# most_used_words_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split by languages\n",
    "\n",
    "We can split the journal entries by language and use the number of minutes as separation. It is split in sentence by sentence basis. If an entry needs to be broken into several languages entries, then each entry is added a number of minutes equal to the index of the sentence within the entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir =  JOURNALS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the journal by languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_language_groupby = split_journal_by_language(journal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the different languages in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_journal_splits_by_language_to_latex(journal_df,filedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The files in which it has been saved to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_journal_splits_by_language_filepaths(filedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload and join the divided journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_journals = load_journals_splitted_by_language(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and save the reloaded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_journals = get_sentences(reloaded_journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = save_cleaned_sentences_to_latex(reloaded_journals, EXAMPLE_CLEANED_JOURNAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
